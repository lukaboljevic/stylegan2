import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from .proxy import proxy


"""
DiscriminatorLoss and GeneratorLoss copied from:
    https://github.com/ppeetteerrs/stylegan2-torch/blob/main/stylegan2_torch/loss.py
as the original StyleGAN2-ADA implementation also used losses from original GAN:
    https://github.com/NVlabs/stylegan2-ada-pytorch/blob/main/training/loss.py

GradientPenalty and PathLengthRegularization (originally named PathLengthPenalty) copied from
    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/gan/stylegan/__init__.py
"""


class DiscriminatorLoss(nn.Module):
    def __init__(self):
        """
        Discriminator loss from the original GAN paper (utilized from StyleGAN onwards)
        """
        super().__init__()

    def forward(self, real_cls, fake_cls):
        """
        Parameters
        ----------
        `real_cls` : Input tensor of shape [batch_size, 1], corresponding to the discriminator
            classification of real images - i.e. images taken from the training dataset
        `fake_cls` : Input tensor of shape [batch_size, 1], corresponding to the discriminator
            classification of fake images - i.e. images generated by the generator

        Returns
        -------
        `real_loss` : The part of discriminator loss corresponding to the real input
        `fake_loss` : The part of discriminator loss corresponding to the fake input

        The two losses are returned as a tuple solely for logging purposes. They should be summed
        to obtain the actual discriminator loss.
        """
        real_loss = F.softplus(-real_cls)  # log(D(x)) from original GAN; equals -log(sigmoid(real_cls))
        fake_loss = F.softplus(fake_cls)  # log(1 - D(G(z))) from original GAN; equals -log(sigmoid(-fake_cls))

        # Return separately instead of adding solely for logging purposes
        return real_loss.mean(), fake_loss.mean()

    __call__ = proxy(forward)


class GeneratorLoss(nn.Module):
    def __init__(self):
        """
        Non-saturating logistic generator loss from the original GAN paper (stated just above Figure 1)
        """
        super().__init__()

    def forward(self, fake_cls):
        """
        Parameters
        ----------
        `fake_cls` : Input tensor of shape [batch_size, 1], corresponding to the discriminator
            classification of fake images - i.e. images generated by the generator

        Returns
        -------
        `fake_loss` : Non-saturating logistic generator loss from original GAN paper
        """
        fake_loss = F.softplus(-fake_cls)  # -log(sigmoid(fake_cls))
        return fake_loss.mean()

    __call__ = proxy(forward)


class GradientPenalty(nn.Module):
    def __init__(self):
        """
        The R1 regularization used alongside discriminator loss (utilized from StyleGAN onwards)
        """
        super().__init__()

    def forward(self, real_images, real_cls):
        """
        Parameters
        ----------
        `real_images` : Tensor of shape [batch_size, 3, 64, 64], corresponding to real images from
            the training dataset
        `real_cls` : Tensor of shape [batch_size, 1], corresponding to the discriminator output i.e.
            classification of `real_images`

        Returns
        -------
        `gradient_penalty` : Gradient penalty to add to the discriminator loss (not multiplied by gamma)
        """
        batch_size = real_images.shape[0]

        gradients, *_ = torch.autograd.grad(
            outputs=real_cls, inputs=real_images, grad_outputs=real_cls.new_ones(real_cls.shape), create_graph=True
        )

        gradients = gradients.reshape(batch_size, -1)
        norm = gradients.norm(2, dim=-1)
        return torch.mean(norm**2)

    __call__ = proxy(forward)


class PathLengthRegularization(nn.Module):
    def __init__(self, beta=0.99):
        """
        Path length regularization used alongside generator loss, introduced in StyleGAN2
        """
        super().__init__()

        self.beta = beta
        self.steps = nn.Parameter(torch.tensor(0.0), requires_grad=False)
        self.exp_sum_a = nn.Parameter(torch.tensor(0.0), requires_grad=False)

    def forward(self, w, generator_output):
        """
        Parameters
        ----------
        `w` : Intermediate latent variable coming from the mapping network of shape
            [batch_size, dim_latent]
        `generator_output` : Images generated by the generator, shape is [batch_size, 3, height, width].
            At the moment, height = width = 64

        Returns
        -------
        `path_length_penalty` : Path length penalty to add to the generator loss, or 0 if `a` can't
            be calculated (self.steps == 0).
        """
        device = generator_output.device

        image_size = generator_output.shape[2] * generator_output.shape[3]

        y = torch.randn_like(generator_output, device=device)
        output = (generator_output * y).sum() / math.sqrt(image_size)

        gradients, *_ = torch.autograd.grad(
            outputs=output,
            inputs=w,
            grad_outputs=torch.ones(output.shape, device=device),
            create_graph=True,
        )
        norm = (gradients**2).sum(dim=1)
        norm = norm.mean(dim=0).sqrt()

        if self.steps > 0:
            a = self.exp_sum_a / (1 - self.beta**self.steps)
            loss = torch.mean((norm - a) ** 2)
        else:
            loss = norm.new_tensor(0)

        mean = norm.mean().detach()
        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)
        self.steps.add_(1.0)

        return loss

    __call__ = proxy(forward)
